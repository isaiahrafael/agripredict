Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.99 GB / 32.00 GB (56.2%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 15 rows, 15 time series. Median time series length is 1 (min=1, max=1). 
	Removing 15 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.84 GB / 32.00 GB (55.8%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.84 GB / 32.00 GB (55.8%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 15 rows, 15 time series. Median time series length is 1 (min=1, max=1). 
Provided train_data has 15 rows, 15 time series. Median time series length is 1 (min=1, max=1). 
	Removing 15 short time series from train_data. Only series with length >= 6 will be used for training.
	Removing 15 short time series from train_data. Only series with length >= 6 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.83 GB / 32.00 GB (55.7%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.83 GB / 32.00 GB (55.7%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.83 GB / 32.00 GB (55.7%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 30,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 30,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 30,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 197 short time series from train_data. Only series with length >= 61 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 61 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 61 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.78 GB / 32.00 GB (55.6%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.78 GB / 32.00 GB (55.6%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.78 GB / 32.00 GB (55.6%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.78 GB / 32.00 GB (55.6%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 25,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 25,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 25,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 25,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 197 short time series from train_data. Only series with length >= 51 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 51 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 51 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 51 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.76 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.76 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.76 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.76 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.76 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 197 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 41 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.75 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.75 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.75 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.75 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.75 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.75 GB / 32.00 GB (55.5%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 197 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 37 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 35 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.74 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 197 short time series from train_data. Only series with length >= 33 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       17.72 GB / 32.00 GB (55.4%)
Disk Space Avail:   838.98 GB / 931.55 GB (90.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
Provided train_data has 5798 rows (NaN fraction=27.0%), 197 time series. Median time series length is 30 (min=25, max=31). 
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 154 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51

Starting training. Start time is 2024-07-31 15:53:51
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
Training timeseries model Naive. Training for up to 149.1s of the 1192.5s of remaining time.
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	-1.3230       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
	2.90    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 169.9s of the 1189.6s of remaining time.
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	-1.4628       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 198.2s of the 1189.5s of remaining time.
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 237.0s of the 1184.9s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
Training timeseries model ETS. Training for up to 296.2s of the 1184.6s of remaining time.
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	-1.4692       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
	29.64   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
Training timeseries model Theta. Training for up to 385.0s of the 1154.9s of remaining time.
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	-1.4350       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
	11.29   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 571.8s of the 1143.6s of remaining time.
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	-1.8512       = Validation score (-MASE)
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	122.77  s     = Training runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	Ensemble weights: {'Naive': 0.85, 'SeasonalNaive': 0.08, 'TemporalFusionTransformer': 0.06}
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	-1.3127       = Validation score (-MASE)
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	0.34    s     = Training runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
	3.02    s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Total runtime: 178.79 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
Best model score: -1.3127
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.35 GB / 32.00 GB (48.0%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 41 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.33 GB / 32.00 GB (47.9%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.33 GB / 32.00 GB (47.9%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 39 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.28 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.28 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.28 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 37 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.29 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.29 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.29 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.29 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.30 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.30 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.30 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.30 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.30 GB / 32.00 GB (47.8%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.27 GB / 32.00 GB (47.7%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.27 GB / 32.00 GB (47.7%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.27 GB / 32.00 GB (47.7%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.27 GB / 32.00 GB (47.7%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.27 GB / 32.00 GB (47.7%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.27 GB / 32.00 GB (47.7%)
Disk Space Avail:   838.64 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 16:34:47

Starting training. Start time is 2024-07-31 16:34:47

Starting training. Start time is 2024-07-31 16:34:47

Starting training. Start time is 2024-07-31 16:34:47

Starting training. Start time is 2024-07-31 16:34:47

Starting training. Start time is 2024-07-31 16:34:47
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.9s of the 1199.1s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.1s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.1s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.1s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.1s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.1s of remaining time.
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.65    s     = Validation (prediction) runtime
	2.65    s     = Validation (prediction) runtime
	2.65    s     = Validation (prediction) runtime
	2.65    s     = Validation (prediction) runtime
	2.65    s     = Validation (prediction) runtime
	2.65    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.4s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.4s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.4s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.4s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.4s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.4s of remaining time.
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.3s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.3s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.3s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.3s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.3s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.3s of remaining time.
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 238.8s of the 1193.8s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.8s of the 1193.8s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.8s of the 1193.8s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.8s of the 1193.8s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.8s of the 1193.8s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.8s of the 1193.8s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 298.4s of the 1193.5s of remaining time.
Training timeseries model ETS. Training for up to 298.4s of the 1193.5s of remaining time.
Training timeseries model ETS. Training for up to 298.4s of the 1193.5s of remaining time.
Training timeseries model ETS. Training for up to 298.4s of the 1193.5s of remaining time.
Training timeseries model ETS. Training for up to 298.4s of the 1193.5s of remaining time.
Training timeseries model ETS. Training for up to 298.4s of the 1193.5s of remaining time.
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	30.11   s     = Validation (prediction) runtime
	30.11   s     = Validation (prediction) runtime
	30.11   s     = Validation (prediction) runtime
	30.11   s     = Validation (prediction) runtime
	30.11   s     = Validation (prediction) runtime
	30.11   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	10.94   s     = Validation (prediction) runtime
	10.94   s     = Validation (prediction) runtime
	10.94   s     = Validation (prediction) runtime
	10.94   s     = Validation (prediction) runtime
	10.94   s     = Validation (prediction) runtime
	10.94   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 576.2s of the 1152.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 576.2s of the 1152.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 576.2s of the 1152.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 576.2s of the 1152.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 576.2s of the 1152.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 576.2s of the 1152.3s of remaining time.
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	92.07   s     = Training runtime
	92.07   s     = Training runtime
	92.07   s     = Training runtime
	92.07   s     = Training runtime
	92.07   s     = Training runtime
	92.07   s     = Training runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	0.29    s     = Training runtime
	0.29    s     = Training runtime
	0.29    s     = Training runtime
	0.29    s     = Training runtime
	0.29    s     = Training runtime
	0.29    s     = Training runtime
	43.70   s     = Validation (prediction) runtime
	43.70   s     = Validation (prediction) runtime
	43.70   s     = Validation (prediction) runtime
	43.70   s     = Validation (prediction) runtime
	43.70   s     = Validation (prediction) runtime
	43.70   s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 139.74 s
Total runtime: 139.74 s
Total runtime: 139.74 s
Total runtime: 139.74 s
Total runtime: 139.74 s
Total runtime: 139.74 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.47 GB / 32.00 GB (48.4%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 41 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.46 GB / 32.00 GB (48.3%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.46 GB / 32.00 GB (48.3%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 18,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 37 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 37 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.48 GB / 32.00 GB (48.4%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.48 GB / 32.00 GB (48.4%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.48 GB / 32.00 GB (48.4%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 17,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 35 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.44 GB / 32.00 GB (48.3%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.44 GB / 32.00 GB (48.3%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.44 GB / 32.00 GB (48.3%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.44 GB / 32.00 GB (48.3%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 16,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	Removing 199 short time series from train_data. Only series with length >= 33 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Warning: path already exists! This predictor may overwrite an existing predictor! path="autogloun-daily"
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.42 GB / 32.00 GB (48.2%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.42 GB / 32.00 GB (48.2%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.42 GB / 32.00 GB (48.2%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.42 GB / 32.00 GB (48.2%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       15.42 GB / 32.00 GB (48.2%)
Disk Space Avail:   838.63 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
Provided train_data has 5845 rows (NaN fraction=27.0%), 199 time series. Median time series length is 30 (min=12, max=31). 
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 156 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1333 rows (NaN fraction=26.5%), 43 time series. Median time series length is 31 (min=31, max=31). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 16:46:11

Starting training. Start time is 2024-07-31 16:46:11

Starting training. Start time is 2024-07-31 16:46:11

Starting training. Start time is 2024-07-31 16:46:11

Starting training. Start time is 2024-07-31 16:46:11
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.8s of the 1198.7s of remaining time.
Training timeseries model Naive. Training for up to 149.8s of the 1198.7s of remaining time.
Training timeseries model Naive. Training for up to 149.8s of the 1198.7s of remaining time.
Training timeseries model Naive. Training for up to 149.8s of the 1198.7s of remaining time.
Training timeseries model Naive. Training for up to 149.8s of the 1198.7s of remaining time.
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	-1.0905       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.38    s     = Validation (prediction) runtime
	2.38    s     = Validation (prediction) runtime
	2.38    s     = Validation (prediction) runtime
	2.38    s     = Validation (prediction) runtime
	2.38    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.3s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.3s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.3s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.3s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.3s of remaining time.
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	-1.7395       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.4s of the 1196.1s of remaining time.
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 238.6s of the 1192.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.6s of the 1192.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.6s of the 1192.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.6s of the 1192.9s of remaining time.
Training timeseries model DirectTabular. Training for up to 238.6s of the 1192.9s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 298.1s of the 1192.6s of remaining time.
Training timeseries model ETS. Training for up to 298.1s of the 1192.6s of remaining time.
Training timeseries model ETS. Training for up to 298.1s of the 1192.6s of remaining time.
Training timeseries model ETS. Training for up to 298.1s of the 1192.6s of remaining time.
Training timeseries model ETS. Training for up to 298.1s of the 1192.6s of remaining time.
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	-1.1177       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	31.02   s     = Validation (prediction) runtime
	31.02   s     = Validation (prediction) runtime
	31.02   s     = Validation (prediction) runtime
	31.02   s     = Validation (prediction) runtime
	31.02   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 387.2s of the 1161.5s of remaining time.
Training timeseries model Theta. Training for up to 387.2s of the 1161.5s of remaining time.
Training timeseries model Theta. Training for up to 387.2s of the 1161.5s of remaining time.
Training timeseries model Theta. Training for up to 387.2s of the 1161.5s of remaining time.
Training timeseries model Theta. Training for up to 387.2s of the 1161.5s of remaining time.
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	-1.2388       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	10.93   s     = Validation (prediction) runtime
	10.93   s     = Validation (prediction) runtime
	10.93   s     = Validation (prediction) runtime
	10.93   s     = Validation (prediction) runtime
	10.93   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 575.3s of the 1150.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.3s of the 1150.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.3s of the 1150.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.3s of the 1150.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.3s of the 1150.5s of remaining time.
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	-3.0294       = Validation score (-MASE)
	100.47  s     = Training runtime
	100.47  s     = Training runtime
	100.47  s     = Training runtime
	100.47  s     = Training runtime
	100.47  s     = Training runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	Ensemble weights: {'ETS': 0.42, 'Naive': 0.56, 'Theta': 0.02}
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	-1.0522       = Validation score (-MASE)
	0.38    s     = Training runtime
	0.38    s     = Training runtime
	0.38    s     = Training runtime
	0.38    s     = Training runtime
	0.38    s     = Training runtime
	44.32   s     = Validation (prediction) runtime
	44.32   s     = Validation (prediction) runtime
	44.32   s     = Validation (prediction) runtime
	44.32   s     = Validation (prediction) runtime
	44.32   s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 149.53 s
Total runtime: 149.53 s
Total runtime: 149.53 s
Total runtime: 149.53 s
Total runtime: 149.53 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
Best model score: -1.0522
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 20,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
	Removing 284 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 41 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 41 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 19,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	Removing 284 short time series from train_data. Only series with length >= 39 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.92 GB / 32.00 GB (46.6%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
Provided train_data has 8039 rows (NaN fraction=47.0%), 284 time series. Median time series length is 29 (min=2, max=31). 
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 253 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 961 rows (NaN fraction=45.0%), 31 time series. Median time series length is 31 (min=31, max=31). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20

Starting training. Start time is 2024-07-31 17:00:20
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	-1.4981       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
	2.78    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.2s of remaining time.
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	-2.0097       = Validation score (-MASE)
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
	0.08    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.1s of remaining time.
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.1s of the 1195.5s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
Training timeseries model ETS. Training for up to 298.7s of the 1195.0s of remaining time.
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	-1.5543       = Validation score (-MASE)
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	0.06    s     = Training runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
	29.37   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
Training timeseries model Theta. Training for up to 388.5s of the 1165.5s of remaining time.
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	-1.6017       = Validation score (-MASE)
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
	14.04   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 575.7s of the 1151.5s of remaining time.
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	-2.8631       = Validation score (-MASE)
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	100.62  s     = Training runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
	0.06    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	Ensemble weights: {'Naive': 0.51, 'SeasonalNaive': 0.14, 'Theta': 0.35}
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	-1.4780       = Validation score (-MASE)
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
	16.90   s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 148.62 s
Total runtime: 148.62 s
Total runtime: 148.62 s
Total runtime: 148.62 s
Total runtime: 148.62 s
Total runtime: 148.62 s
Total runtime: 148.62 s
Total runtime: 148.62 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -1.4780
Best model score: -1.4780
Best model score: -1.4780
Best model score: -1.4780
Best model score: -1.4780
Best model score: -1.4780
Best model score: -1.4780
Best model score: -1.4780
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.89 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.62 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46

Starting training. Start time is 2024-07-31 17:04:46
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
	2.97    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.0s of remaining time.
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.03    s     = Training runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
	0.09    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1195.9s of remaining time.
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.2s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.5s of remaining time.
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	0.09    s     = Training runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
	28.19   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.2s of remaining time.
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
	11.73   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.2s of the 1154.4s of remaining time.
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	92.60   s     = Training runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
	42.89   s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Total runtime: 137.66 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.50 GB / 32.00 GB (45.3%)
Disk Space Avail:   838.58 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 2,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 6808 rows (NaN fraction=29.0%), 19 time series. Median time series length is 364 (min=269, max=365). 

Provided data contains following columns:
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2024-07-31 17:41:05
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.7s of the 1197.6s of remaining time.
	-0.7567       = Validation score (-MASE)
	0.03    s     = Training runtime
	2.60    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 170.7s of the 1194.9s of remaining time.
	-1.5383       = Validation score (-MASE)
	0.04    s     = Training runtime
	0.04    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.1s of the 1194.8s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 238.4s of the 1191.8s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 297.8s of the 1191.3s of remaining time.
	-0.7596       = Validation score (-MASE)
	0.09    s     = Training runtime
	27.96   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 387.8s of the 1163.3s of remaining time.
	-0.7427       = Validation score (-MASE)
	0.04    s     = Training runtime
	13.48   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 574.9s of the 1149.7s of remaining time.
	-1.8449       = Validation score (-MASE)
	8.57    s     = Training runtime
	0.05    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
	Ensemble weights: {'Theta': 1.0}
	-0.7427       = Validation score (-MASE)
	0.25    s     = Training runtime
	13.48   s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 57.13 s
Best model: Theta
Best model score: -0.7427
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.69 GB / 32.00 GB (45.9%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.69 GB / 32.00 GB (45.9%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'A-DEC',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 2,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'A-DEC',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 2,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'A-DEC'.
train_data with frequency 'None' has been resampled to frequency 'A-DEC'.
Provided train_data has 19 rows, 19 time series. Median time series length is 1 (min=1, max=1). 
Provided train_data has 19 rows, 19 time series. Median time series length is 1 (min=1, max=1). 
	Removing 19 short time series from train_data. Only series with length >= 7 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 7 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.57 GB / 32.00 GB (45.5%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.57 GB / 32.00 GB (45.5%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.57 GB / 32.00 GB (45.5%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'A-DEC',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'A-DEC',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'A-DEC',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'A-DEC'.
train_data with frequency 'None' has been resampled to frequency 'A-DEC'.
train_data with frequency 'None' has been resampled to frequency 'A-DEC'.
Provided train_data has 19 rows, 19 time series. Median time series length is 1 (min=1, max=1). 
Provided train_data has 19 rows, 19 time series. Median time series length is 1 (min=1, max=1). 
Provided train_data has 19 rows, 19 time series. Median time series length is 1 (min=1, max=1). 
	Removing 19 short time series from train_data. Only series with length >= 6 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 6 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 6 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.71 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.71 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.71 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.71 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 17:43:25

Starting training. Start time is 2024-07-31 17:43:25

Starting training. Start time is 2024-07-31 17:43:25

Starting training. Start time is 2024-07-31 17:43:25
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 150.0s of the 1199.8s of remaining time.
Training timeseries model Naive. Training for up to 150.0s of the 1199.8s of remaining time.
Training timeseries model Naive. Training for up to 150.0s of the 1199.8s of remaining time.
Training timeseries model Naive. Training for up to 150.0s of the 1199.8s of remaining time.
	-2.7987       = Validation score (-MASE)
	-2.7987       = Validation score (-MASE)
	-2.7987       = Validation score (-MASE)
	-2.7987       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.77    s     = Validation (prediction) runtime
	2.77    s     = Validation (prediction) runtime
	2.77    s     = Validation (prediction) runtime
	2.77    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1197.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1197.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1197.0s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1197.0s of remaining time.
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.88 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.88 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.88 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.88 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.88 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 6,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 6,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 6,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 6,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 6,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
	Removing 19 short time series from train_data. Only series with length >= 13 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 13 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 13 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 13 will be used for training.
	Removing 19 short time series from train_data. Only series with length >= 13 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
train_data with frequency 'None' has been resampled to frequency 'M'.
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 17:43:55

Starting training. Start time is 2024-07-31 17:43:55

Starting training. Start time is 2024-07-31 17:43:55

Starting training. Start time is 2024-07-31 17:43:55

Starting training. Start time is 2024-07-31 17:43:55

Starting training. Start time is 2024-07-31 17:43:55
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	2.46    s     = Validation (prediction) runtime
	2.46    s     = Validation (prediction) runtime
	2.46    s     = Validation (prediction) runtime
	2.46    s     = Validation (prediction) runtime
	2.46    s     = Validation (prediction) runtime
	2.46    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	-5.1694       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.05    s     = Validation (prediction) runtime
	0.05    s     = Validation (prediction) runtime
	0.05    s     = Validation (prediction) runtime
	0.05    s     = Validation (prediction) runtime
	0.05    s     = Validation (prediction) runtime
	0.05    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	-6.1682       = Validation score (-MASE)
	-6.1682       = Validation score (-MASE)
	-6.1682       = Validation score (-MASE)
	-6.1682       = Validation score (-MASE)
	-6.1682       = Validation score (-MASE)
	-6.1682       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	22.25   s     = Validation (prediction) runtime
	22.25   s     = Validation (prediction) runtime
	22.25   s     = Validation (prediction) runtime
	22.25   s     = Validation (prediction) runtime
	22.25   s     = Validation (prediction) runtime
	22.25   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
	-6.0635       = Validation score (-MASE)
	-6.0635       = Validation score (-MASE)
	-6.0635       = Validation score (-MASE)
	-6.0635       = Validation score (-MASE)
	-6.0635       = Validation score (-MASE)
	-6.0635       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	11.32   s     = Validation (prediction) runtime
	11.32   s     = Validation (prediction) runtime
	11.32   s     = Validation (prediction) runtime
	11.32   s     = Validation (prediction) runtime
	11.32   s     = Validation (prediction) runtime
	11.32   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
	-6.9462       = Validation score (-MASE)
	-6.9462       = Validation score (-MASE)
	-6.9462       = Validation score (-MASE)
	-6.9462       = Validation score (-MASE)
	-6.9462       = Validation score (-MASE)
	-6.9462       = Validation score (-MASE)
	72.59   s     = Training runtime
	72.59   s     = Training runtime
	72.59   s     = Training runtime
	72.59   s     = Training runtime
	72.59   s     = Training runtime
	72.59   s     = Training runtime
	0.03    s     = Validation (prediction) runtime
	0.03    s     = Validation (prediction) runtime
	0.03    s     = Validation (prediction) runtime
	0.03    s     = Validation (prediction) runtime
	0.03    s     = Validation (prediction) runtime
	0.03    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	-5.1504       = Validation score (-MASE)
	-5.1504       = Validation score (-MASE)
	-5.1504       = Validation score (-MASE)
	-5.1504       = Validation score (-MASE)
	-5.1504       = Validation score (-MASE)
	-5.1504       = Validation score (-MASE)
	0.25    s     = Training runtime
	0.25    s     = Training runtime
	0.25    s     = Training runtime
	0.25    s     = Training runtime
	0.25    s     = Training runtime
	0.25    s     = Training runtime
	2.49    s     = Validation (prediction) runtime
	2.49    s     = Validation (prediction) runtime
	2.49    s     = Validation (prediction) runtime
	2.49    s     = Validation (prediction) runtime
	2.49    s     = Validation (prediction) runtime
	2.49    s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 110.01 s
Total runtime: 110.01 s
Total runtime: 110.01 s
Total runtime: 110.01 s
Total runtime: 110.01 s
Total runtime: 110.01 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -5.1504
Best model score: -5.1504
Best model score: -5.1504
Best model score: -5.1504
Best model score: -5.1504
Best model score: -5.1504
data with frequency 'None' has been resampled to frequency 'M'.
data with frequency 'None' has been resampled to frequency 'M'.
data with frequency 'None' has been resampled to frequency 'M'.
data with frequency 'None' has been resampled to frequency 'M'.
data with frequency 'None' has been resampled to frequency 'M'.
data with frequency 'None' has been resampled to frequency 'M'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.46 GB / 32.00 GB (45.2%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'D',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 15,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
train_data with frequency 'None' has been resampled to frequency 'D'.
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
Provided train_data has 6679 rows (NaN fraction=27.0%), 227 time series. Median time series length is 30 (min=19, max=31). 
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	Removing 177 short time series from train_data. Only series with length >= 31 will be used for training.
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 
	After filtering, train_data has 1550 rows (NaN fraction=26.4%), 50 time series. Median time series length is 31 (min=31, max=31). 

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46

Starting training. Start time is 2024-07-31 17:52:46
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
Training timeseries model Naive. Training for up to 149.9s of the 1199.0s of remaining time.
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	-1.0541       = Validation score (-MASE)
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	0.04    s     = Training runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
	2.82    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
Training timeseries model SeasonalNaive. Training for up to 170.9s of the 1196.1s of remaining time.
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	-1.7519       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.02    s     = Training runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
	0.12    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
Training timeseries model RecursiveTabular. Training for up to 199.3s of the 1196.0s of remaining time.
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Time series in the dataset are too short for chosen differences [7]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
Training timeseries model DirectTabular. Training for up to 239.0s of the 1195.1s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
Training timeseries model ETS. Training for up to 298.6s of the 1194.3s of remaining time.
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	-1.0753       = Validation score (-MASE)
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	0.07    s     = Training runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
	28.31   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
Training timeseries model Theta. Training for up to 388.7s of the 1166.0s of remaining time.
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	-1.1936       = Validation score (-MASE)
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	0.05    s     = Training runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
	10.67   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
Training timeseries model TemporalFusionTransformer. Training for up to 577.6s of the 1155.2s of remaining time.
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	-3.0299       = Validation score (-MASE)
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	76.76   s     = Training runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
	0.07    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
Fitting simple weighted ensemble.
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	Ensemble weights: {'ETS': 0.41, 'Naive': 0.55, 'Theta': 0.04}
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	-1.0201       = Validation score (-MASE)
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	0.32    s     = Training runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
	41.80   s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Total runtime: 121.00 s
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model: WeightedEnsemble
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
Best model score: -1.0201
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: Theta/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
	Warning: ETS/W0 failed for 1 time series (0.4%). Fallback model SeasonalNaive was used for these time series.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
data with frequency 'None' has been resampled to frequency 'D'.
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).
