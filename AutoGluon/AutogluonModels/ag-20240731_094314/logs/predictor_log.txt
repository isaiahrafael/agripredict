Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.71 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.57 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 1,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'M'.
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 

Provided data contains following columns:
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2024-07-31 17:43:25
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 150.0s of the 1199.8s of remaining time.
	-2.7987       = Validation score (-MASE)
	0.02    s     = Training runtime
	2.77    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1197.0s of remaining time.
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.88 GB / 32.00 GB (46.5%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 6,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'M'.
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 
	Removing 19 short time series from train_data. Only series with length >= 13 will be used for training.
	After filtering, train_data has 0 rows, 0 time series. Median time series length is nan (min=nan, max=nan). 
Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'autogloun-daily'
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.7
Operating System:   Darwin
Platform Machine:   x86_64
Platform Version:   Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
CPU Count:          8
GPU Count:          0
Memory Avail:       14.72 GB / 32.00 GB (46.0%)
Disk Space Avail:   838.56 GB / 931.55 GB (90.0%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MASE,
 'freq': 'M',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 1,
 'prediction_length': 3,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 1200,
 'verbosity': 2}

train_data with frequency 'None' has been resampled to frequency 'M'.
Provided train_data has 225 rows, 19 time series. Median time series length is 12 (min=9, max=12). 

Provided data contains following columns:
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MASE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2024-07-31 17:43:55
Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'TemporalFusionTransformer']
Training timeseries model Naive. Training for up to 149.9s of the 1199.3s of remaining time.
	-5.1694       = Validation score (-MASE)
	0.02    s     = Training runtime
	2.46    s     = Validation (prediction) runtime
Training timeseries model SeasonalNaive. Training for up to 171.0s of the 1196.9s of remaining time.
	-5.1694       = Validation score (-MASE)
	0.02    s     = Training runtime
	0.05    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 199.5s of the 1196.8s of remaining time.
	Time series in the dataset are too short for chosen differences [12]. Setting differences to [1].
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused RecursiveTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
Training timeseries model DirectTabular. Training for up to 239.3s of the 1196.3s of remaining time.
	Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
		`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /opt/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so
  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)
	Warning: Exception caused DirectTabular to fail during training... Skipping this model.
	Trainer has no fit models that can infer.
Training timeseries model ETS. Training for up to 299.0s of the 1195.9s of remaining time.
	Warning: ETS/W0 failed for 1 time series (5.3%). Fallback model SeasonalNaive was used for these time series.
	-6.1682       = Validation score (-MASE)
	0.02    s     = Training runtime
	22.25   s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 391.2s of the 1173.6s of remaining time.
	-6.0635       = Validation score (-MASE)
	0.02    s     = Training runtime
	11.32   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 581.1s of the 1162.3s of remaining time.
	-6.9462       = Validation score (-MASE)
	72.59   s     = Training runtime
	0.03    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
	Ensemble weights: {'Naive': 0.88, 'TemporalFusionTransformer': 0.12}
	-5.1504       = Validation score (-MASE)
	0.25    s     = Training runtime
	2.49    s     = Validation (prediction) runtime
Training complete. Models trained: ['Naive', 'SeasonalNaive', 'ETS', 'Theta', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 110.01 s
Best model: WeightedEnsemble
Best model score: -5.1504
data with frequency 'None' has been resampled to frequency 'M'.
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
